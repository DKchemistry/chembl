{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from rdkit import Chem\n",
    "from matplotlib.cm import viridis\n",
    "from matplotlib.colors import Normalize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Graphing Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcdefaults()\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Papermill Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# title suffix\n",
    "title_suffix = \"ADRB2_4lde\"\n",
    "\n",
    "# Files we are processing\n",
    "file_path_sdf_active = \"/Users/lkv206/work/to_do_projects/chembl_ligands/grids_lit-pcba/ADRB2/ADRB2_4lde_active_docking_lib_sorted.sdf\"\n",
    "file_path_sdf_decoy = \"/Users/lkv206/work/to_do_projects/chembl_ligands/grids_lit-pcba/ADRB2/ADRB2_4lde_inactive_docking_lib_sorted.sdf\"\n",
    "\n",
    "file_path_strain_active = \"/Users/lkv206/work/to_do_projects/chembl_ligands/grids_lit-pcba/ADRB2/strain/ADRB2_4lde_active_docking_lib_sorted_strain.csv\"\n",
    "file_path_strain_decoy = \"/Users/lkv206/work/to_do_projects/chembl_ligands/grids_lit-pcba/ADRB2/strain/ADRB2_4lde_inactive_docking_lib_sorted_strain.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdf_to_df(args):\n",
    "    \"\"\"\n",
    "    Load molecules and their properties from an SDF file into a DataFrame.\n",
    "\n",
    "    Example usage: df=sdf_to_df((\"./ADRB1/docking/ADRB1_active_docking_lib.sdf\", \"active\"))\n",
    "\n",
    "    Note that this function was originally intended to be used with the `multiprocessing` module, so the arguments are passed as a tuple.\n",
    "\n",
    "    Will likely be updated later.\n",
    "\n",
    "    \"\"\"\n",
    "    file, active_status = args  # Unpack the tuple of arguments\n",
    "\n",
    "    # Create a molecule supplier\n",
    "    mol_supplier = Chem.MultithreadedSDMolSupplier(file, numWriterThreads=8)\n",
    "\n",
    "    # Load the molecules and their properties into a list\n",
    "    molecules = []\n",
    "    first_mol = True\n",
    "    for mol in mol_supplier:\n",
    "        if mol is not None:\n",
    "            if first_mol:\n",
    "                # Get properties as dictionary only for the first molecule\n",
    "                props = mol.GetPropsAsDict()\n",
    "                keys = props.keys()\n",
    "                first_mol = False\n",
    "            else:\n",
    "                # For the rest of the molecules, get properties directly\n",
    "                props = {key: mol.GetProp(key) for key in keys if mol.HasProp(key)}\n",
    "\n",
    "            props[\"Title\"] = mol.GetProp(\"_Name\")\n",
    "            props[\"Mol\"] = mol\n",
    "            props[\"Activity\"] = 1 if active_status == \"active\" else 0\n",
    "            molecules.append(props)\n",
    "\n",
    "    # Convert the list into a DataFrame\n",
    "    df = pd.DataFrame(molecules)\n",
    "\n",
    "    # Reorder the DataFrame columns\n",
    "    cols = [\"Title\", \"Mol\", \"Activity\"] + [\n",
    "        col for col in df.columns if col not in [\"Title\", \"Mol\", \"Activity\"]\n",
    "    ]\n",
    "    df = df[cols]\n",
    "    df = df.rename(columns={\"Title\": \"Molecule_Name\"})\n",
    "\n",
    "    # Convert 'r_i_docking_score' to numeric, coercing errors to NaN\n",
    "    df[\"r_i_docking_score\"] = pd.to_numeric(df[\"r_i_docking_score\"], errors=\"coerce\")\n",
    "\n",
    "    # Print 'Molecule_Name' and 'r_i_docking_score' for entries that could not be converted\n",
    "    non_convertible_entries = df[df[\"r_i_docking_score\"].isna()]\n",
    "    for _, row in non_convertible_entries.iterrows():\n",
    "        print(\n",
    "            f\"Molecule_Name: {row['Molecule_Name']}, r_i_docking_score: {row['r_i_docking_score']}\"\n",
    "        )\n",
    "\n",
    "    # Drop rows with non-convertible 'r_i_docking_score'\n",
    "    df = df.dropna(subset=[\"r_i_docking_score\"])\n",
    "\n",
    "    # Convert 'r_i_docking_score' to int64\n",
    "    df[\"r_i_docking_score\"] = df[\"r_i_docking_score\"].astype(\"float64\")\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_sdf = sdf_to_df((file_path_sdf_active, \"active\"))\n",
    "decoy_sdf = sdf_to_df((file_path_sdf_decoy, \"inactive\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_actives = active_sdf[\"Molecule_Name\"].duplicated()\n",
    "print(\"Duplicates in active_sdf:\", any(duplicates_actives))\n",
    "\n",
    "duplicates_decoys = decoy_sdf[\"Molecule_Name\"].duplicated()\n",
    "print(\"Duplicates in decoy_sdf:\", any(duplicates_decoys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path_strain_active = \"/Users/lkv206/work/to_do_projects/chembl_ligands/grids_lit-pcba/ADRB2/strain/ADRB2_4lde_active_docking_lib_sorted_strain.csv\"\n",
    "file_path_strain_decoy = \"/Users/lkv206/work/to_do_projects/chembl_ligands/grids_lit-pcba/ADRB2/strain/ADRB2_4lde_inactive_docking_lib_sorted_strain.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_csv_files(file_list):\n",
    "    \"\"\"\n",
    "    Concatenates multiple strain CSV files into a single dataframe.\n",
    "    Only the first five columns are kept for now.\n",
    "\n",
    "    Args:\n",
    "        file_list (list): A list of file paths to the CSV files.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The concatenated dataframe.\n",
    "\n",
    "    \"\"\"\n",
    "    # Specify the column names\n",
    "    column_names = [\n",
    "        \"Molecule_Name\",\n",
    "        \"Total_E\",\n",
    "        \"Lower_Bound\",\n",
    "        \"Upper_Bound\",\n",
    "        \"Num_Torsion_Patterns\",\n",
    "    ]\n",
    "\n",
    "    # List to hold dataframes\n",
    "    df_list = []\n",
    "\n",
    "    # Loop over each file in the list\n",
    "    for file in file_list:\n",
    "        # Import the CSV file as a df, using only the first five columns of the CSV file\n",
    "        df = pd.read_csv(file, usecols=range(5), names=column_names, header=None)\n",
    "        df_list.append(df)\n",
    "\n",
    "    # Concatenate all dataframes in the list\n",
    "    final_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    return final_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_strain = concatenate_csv_files([file_path_strain_active])\n",
    "decoy_strain = concatenate_csv_files([file_path_strain_decoy])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df, column_name=\"Total_E\"):\n",
    "    initial_count = len(df)\n",
    "    nan_count = df[column_name].isna().sum()\n",
    "    df = df.dropna(subset=[column_name])\n",
    "    negative_count = (df[column_name] < 0).sum()\n",
    "    df = df[df[column_name] >= 0]\n",
    "    print(f\"Dropped {nan_count} rows due to NaN values.\")\n",
    "    print(f\"Dropped {negative_count} rows due to negative values in '{column_name}'.\")\n",
    "    final_count = len(df)\n",
    "    print(f\"Initially {initial_count} rows, now {final_count} rows.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# active_strain = clean_dataframe(active_strain, 'Total_E')\n",
    "# decoy_strain = clean_dataframe(decoy_strain, 'Total_E')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_strain = clean_dataframe(active_strain, \"Total_E\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoy_strain = clean_dataframe(decoy_strain, \"Total_E\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_actives = active_strain[\"Molecule_Name\"].duplicated()\n",
    "print(\"Duplicates in active_strain:\", any(duplicates_actives))\n",
    "\n",
    "duplicates_decoys = decoy_strain[\"Molecule_Name\"].duplicated()\n",
    "print(\"Duplicates in decoy_strain:\", any(duplicates_decoys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_data = pd.merge(active_sdf, active_strain, on=\"Molecule_Name\")\n",
    "decoy_data = pd.merge(decoy_sdf, decoy_strain, on=\"Molecule_Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_merge = {\n",
    "    \"active_sdf\": active_sdf,\n",
    "    \"active_strain\": active_strain,\n",
    "    \"decoy_sdf\": decoy_sdf,\n",
    "    \"decoy_strain\": decoy_strain,\n",
    "}\n",
    "\n",
    "print(\n",
    "    \"Dataframe shapes before the merge of [active_sdf + active_strain] to [active_data] and [decoy_sdf + decoy_strain] to [decoy_data]:\\n\"\n",
    ")\n",
    "for name, df in pre_merge.items():\n",
    "    print(f\"{name}: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_merge = {\"active_data\": active_data, \"decoy_data\": decoy_data}\n",
    "\n",
    "print(\"Dataframe shapes after merge to active_data and decoy_data:\")\n",
    "for name, df in post_merge.items():\n",
    "    print(f\"{name}: {df.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([active_data, decoy_data])\n",
    "\n",
    "print(\n",
    "    \"Dataframe shape after concat of active_data and decoy_data to all_data\\n\",\n",
    "    all_data.shape,\n",
    "    \"\\n\",\n",
    ")\n",
    "\n",
    "print(\"all_data should be addition of the inputs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(all_data[\"r_i_docking_score\"], all_data[\"Total_E\"])\n",
    "actives_all_data = all_data[all_data[\"Activity\"] == 1]\n",
    "plt.scatter(actives_all_data[\"r_i_docking_score\"], actives_all_data[\"Total_E\"])\n",
    "plt.title(f\"r_i_docking_score vs. Total_E ({title_suffix})\")\n",
    "plt.xlabel(\"r_i_docking_score\")\n",
    "plt.ylabel(\"Total_E\")\n",
    "plt.legend([\"Decoys\", \"Actives\"])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_density_docking(df, title_suffix):\n",
    "    # Hardcoded column names\n",
    "    activity_col = \"Activity\"\n",
    "    score_col = \"r_i_docking_score\"\n",
    "\n",
    "    # Create a density plot for the score of active and inactive molecules\n",
    "    sns.kdeplot(df.loc[df[activity_col] == 0, score_col], label=\"Inactive\", fill=True)\n",
    "    sns.kdeplot(df.loc[df[activity_col] == 1, score_col], label=\"Active\", fill=True)\n",
    "\n",
    "    # Add title and labels\n",
    "    plt.title(\n",
    "        f\"Density Plot of Docking Score for Active and Decoy Molecules ({title_suffix})\"\n",
    "    )\n",
    "    plt.xlabel(\"Docking Score\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_density_docking(all_data, title_suffix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram_dscore(df, title_suffix):\n",
    "    # Hardcoded column names\n",
    "    activity_col = \"Activity\"\n",
    "    score_col = \"r_i_docking_score\"\n",
    "\n",
    "    plt.hist(\n",
    "        df.loc[df[activity_col] == 0, score_col],\n",
    "        bins=50,\n",
    "        label=\"Inactive\",\n",
    "        alpha=0.5,\n",
    "        density=True,\n",
    "    )\n",
    "    plt.hist(\n",
    "        df.loc[df[activity_col] == 1, score_col],\n",
    "        bins=50,\n",
    "        label=\"Active\",\n",
    "        alpha=0.5,\n",
    "        density=True,\n",
    "    )\n",
    "\n",
    "    # Add title and labels\n",
    "    plt.title(\n",
    "        f\"Density Histogram of Docking Score for Active and Decoy Molecules ({title_suffix})\"\n",
    "    )\n",
    "    plt.xlabel(\"Docking Score\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram_dscore(all_data, title_suffix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_density_strain(df, title_suffix):\n",
    "    # Hardcoded column names\n",
    "    activity_col = \"Activity\"\n",
    "    score_col = \"Total_E\"\n",
    "\n",
    "    # Create a density plot for the score of active and inactive molecules\n",
    "    sns.kdeplot(df.loc[df[activity_col] == 0, score_col], label=\"Inactive\", fill=True)\n",
    "    sns.kdeplot(df.loc[df[activity_col] == 1, score_col], label=\"Active\", fill=True)\n",
    "\n",
    "    # Add title and labels\n",
    "    plt.title(\n",
    "        f\"Density Plot of Strain Energy for Active and Decoy Molecules ({title_suffix})\"\n",
    "    )\n",
    "    plt.xlabel(\"Total Strain Energy\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_density_strain(all_data, title_suffix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram_strain(df, title_suffix):\n",
    "    # Hardcoded column names\n",
    "    activity_col = \"Activity\"\n",
    "    score_col = \"Total_E\"\n",
    "\n",
    "    plt.hist(\n",
    "        df.loc[df[activity_col] == 0, score_col],\n",
    "        bins=50,\n",
    "        label=\"Inactive\",\n",
    "        alpha=0.5,\n",
    "        density=True,\n",
    "    )\n",
    "    plt.hist(\n",
    "        df.loc[df[activity_col] == 1, score_col],\n",
    "        bins=50,\n",
    "        label=\"Active\",\n",
    "        alpha=0.5,\n",
    "        density=True,\n",
    "    )\n",
    "\n",
    "    # Add title and labels\n",
    "    plt.title(\n",
    "        f\"Density Histogram of Strain Energy for Active and Decoy Molecules ({title_suffix})\"\n",
    "    )\n",
    "    plt.xlabel(\"Total Strain Energy\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# The density parameter is set to True, which means the histogram will show the density (the number of samples in a bin divided by the size of the bin) instead of the raw count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram_strain(all_data, title_suffix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Enrichment by Strain Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_e_thresholds = [\n",
    "    None,\n",
    "    4,\n",
    "    4.5,\n",
    "    5.0,\n",
    "    5.5,\n",
    "    6.0,\n",
    "    7.0,\n",
    "    7.5,\n",
    "    8.0,\n",
    "]  # strain energy thresholds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_enrichment_parameters(df):\n",
    "    \"\"\"\n",
    "    Calculates parameters we will need for Enrichment Plots\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The dataframe to calculate the logAUC for.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The dataframe with the logAUC calculated.\n",
    "    \"\"\"\n",
    "    # Sort the subset by 'r_i_docking_score' in ascending order\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    df.sort_values(by=\"r_i_docking_score\", inplace=True)\n",
    "\n",
    "    # Reset the index so that we are ranking by the docking score\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Adjusting the index to start from 1\n",
    "    df.index += 1\n",
    "\n",
    "    # Calculate the cumulative sum of active compounds\n",
    "    df[\"Cumulative_Actives\"] = df[\"Activity\"].cumsum()\n",
    "\n",
    "    # Calculate the Total Actives\n",
    "    df[\"Total_Actives\"] = df[\"Activity\"].sum()\n",
    "\n",
    "    # Calculate the fraction of identified actives at each row\n",
    "    df[\"Fraction_Actives\"] = df[\"Cumulative_Actives\"] / df[\"Total_Actives\"]\n",
    "\n",
    "    # Calculate the percentage of compounds screened at each row\n",
    "    df[\"Percentage_Screened\"] = df.index / len(df)\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = calculate_enrichment_parameters(all_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrichment_metrics_by_strain(df, total_e_threshold=None):\n",
    "\n",
    "    if total_e_threshold is not None:\n",
    "        df = df[df[\"Total_E\"] <= total_e_threshold]\n",
    "\n",
    "    df = calculate_enrichment_parameters(df).copy()\n",
    "\n",
    "    closest_to_one_percent = df.iloc[\n",
    "        (df[\"Percentage_Screened\"] - 0.01).abs().argsort()[:1]\n",
    "    ]\n",
    "\n",
    "    ef1 = (\n",
    "        closest_to_one_percent[\"Cumulative_Actives\"].values[0]\n",
    "        / closest_to_one_percent[\"Total_Actives\"].values[0]\n",
    "    ) * 100\n",
    "\n",
    "    closest_to_five_percent = df.iloc[\n",
    "        (df[\"Percentage_Screened\"] - 0.05).abs().argsort()[:1]\n",
    "    ]\n",
    "    ef5 = (\n",
    "        closest_to_five_percent[\"Cumulative_Actives\"].values[0]\n",
    "        / closest_to_five_percent[\"Total_Actives\"].values[0]\n",
    "    ) * 100\n",
    "\n",
    "    return ef1, ef5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plot_enrichment_by_strain(data, thresholds, title_suffix):\n",
    "    # Calculate enrichment for each threshold\n",
    "    ef1s, ef5s = zip(\n",
    "        *[enrichment_metrics_by_strain(data, total_e_threshold=t) for t in thresholds]\n",
    "    )\n",
    "\n",
    "    # Create labels for the x-axis\n",
    "    x_labels = [str(t) if t is not None else \"No Cutoff\" for t in thresholds]\n",
    "\n",
    "    # Create an array with the positions of each bar on the x axis\n",
    "    x = np.arange(len(x_labels))\n",
    "\n",
    "    # Set the width of the bars\n",
    "    bar_width = 0.35\n",
    "\n",
    "    plt.bar(x - bar_width / 2, ef1s, bar_width, label=\"EF1%\")\n",
    "    plt.bar(x + bar_width / 2, ef5s, bar_width, label=\"EF5%\")\n",
    "    plt.title(f\"Enrichment Factors by Strain Energy Cutoff ({title_suffix})\")\n",
    "    plt.xlabel(\"Strain Energy Cutoff\")\n",
    "    plt.ylabel(\"Enrichment Factor (%)\")\n",
    "    plt.xticks(x, x_labels)  # Set the position and labels of the xticks\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_plot_enrichment_by_strain(all_data, total_e_thresholds, title_suffix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plot_delta_enrichment_by_strain(data, thresholds, title_suffix):\n",
    "    # Calculate enrichment for each threshold\n",
    "    ef1s, ef5s = zip(\n",
    "        *[enrichment_metrics_by_strain(data, total_e_threshold=t) for t in thresholds]\n",
    "    )\n",
    "\n",
    "    # Calculate differences in enrichment metrics\n",
    "    ef1s_diff = [ef - ef1s[0] for ef in ef1s]\n",
    "    ef5s_diff = [ef - ef5s[0] for ef in ef5s]\n",
    "\n",
    "    # Create labels for the x-axis\n",
    "    x_labels = [str(t) if t is not None else \"No Cutoff\" for t in thresholds]\n",
    "\n",
    "    # Create an array with the positions of each bar on the x axis\n",
    "    x = np.arange(len(x_labels))\n",
    "\n",
    "    # Set the width of the bars\n",
    "    bar_width = 0.35\n",
    "\n",
    "    plt.bar(x - bar_width / 2, ef1s_diff, bar_width, label=\"EF1% Difference\")\n",
    "    plt.bar(x + bar_width / 2, ef5s_diff, bar_width, label=\"EF5% Difference\")\n",
    "    plt.title(\n",
    "        f\"Difference in Enrichment Factors by Strain Energy Cutoff ({title_suffix})\"\n",
    "    )\n",
    "    plt.xlabel(\"Strain Energy Cutoff\")\n",
    "    plt.ylabel(\"Difference in Enrichment Factor (%)\")\n",
    "    plt.xticks(x, x_labels)  # Set the position and labels of the xticks\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_plot_delta_enrichment_by_strain(all_data, total_e_thresholds, title_suffix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_enrichment_curve_by_strain(df, total_e_threshold=None, ax=None, color=\"blue\"):\n",
    "\n",
    "    if total_e_threshold is not None:\n",
    "        df = df[df[\"Total_E\"] <= total_e_threshold]\n",
    "\n",
    "    df = calculate_enrichment_parameters(df).copy()\n",
    "\n",
    "    enrichment_auc = auc(df[\"Percentage_Screened\"], df[\"Fraction_Actives\"])\n",
    "\n",
    "    closest_to_one_percent = df.iloc[\n",
    "        (df[\"Percentage_Screened\"] - 0.01).abs().argsort()[:1]\n",
    "    ]\n",
    "\n",
    "    ef1 = (\n",
    "        closest_to_one_percent[\"Cumulative_Actives\"].values[0]\n",
    "        / closest_to_one_percent[\"Total_Actives\"].values[0]\n",
    "    ) * 100\n",
    "\n",
    "    closest_to_five_percent = df.iloc[\n",
    "        (df[\"Percentage_Screened\"] - 0.05).abs().argsort()[:1]\n",
    "    ]\n",
    "    ef5 = (\n",
    "        closest_to_five_percent[\"Cumulative_Actives\"].values[0]\n",
    "        / closest_to_five_percent[\"Total_Actives\"].values[0]\n",
    "    ) * 100\n",
    "\n",
    "    # Plot the enrichment curve\n",
    "    ax.plot(\n",
    "        df[\"Percentage_Screened\"] * 100,\n",
    "        df[\"Fraction_Actives\"] * 100,\n",
    "        label=\"Threshold: {}\\nEnrichment AUC = {:.2f}\\n(EF1% = {:.1f}%)\\n(EF5% = {:.0f}%)\".format(\n",
    "            total_e_threshold if total_e_threshold is not None else \"N/A\",\n",
    "            enrichment_auc,\n",
    "            ef1,\n",
    "            ef5,\n",
    "        ),\n",
    "        color=color,\n",
    "    )\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "def plot_enrichment_all_thresholds(data, thresholds, title_suffix):\n",
    "    # Create a single plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    # Define a colormap\n",
    "    cmap = cm.get_cmap(\n",
    "        \"viridis\", len(thresholds) - 1\n",
    "    )  # Subtract 1 because the first color is manually set\n",
    "\n",
    "    # Plot semi-log ROC curve for each threshold\n",
    "    for i, t in enumerate(thresholds):\n",
    "        # Filter dataframe based on 'Total_E' threshold\n",
    "        df_filtered = data if t is None else data[data[\"Total_E\"] <= t]\n",
    "\n",
    "        # Set a distinct color for the first threshold\n",
    "        color = (\n",
    "            \"red\" if t is None else cmap(i - 1)\n",
    "        )  # Subtract 1 because the first color is manually set\n",
    "\n",
    "        # Call the function with the filtered data\n",
    "        plot_enrichment_curve_by_strain(\n",
    "            df_filtered, total_e_threshold=t, ax=ax, color=color\n",
    "        )\n",
    "\n",
    "    ax.set_title(f\"Enrichment Curves by Strain Energy Thresholds ({title_suffix})\")\n",
    "    ax.set_xlabel(\"Percent Screened\")\n",
    "    ax.set_ylabel(\"Percent Identified Actives\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot = plot_enrichment_all_thresholds(all_data, total_e_thresholds, title_suffix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_enrichment_metrics(data, thresholds, title_suffix):\n",
    "    # Calculate enrichment for each threshold\n",
    "    ef1s, ef5s = zip(\n",
    "        *[enrichment_metrics_by_strain(data, total_e_threshold=t) for t in thresholds]\n",
    "    )\n",
    "\n",
    "    # Calculate the difference in enrichment metrics\n",
    "    ef1s_diff = [ef - ef1s[0] for ef in ef1s]\n",
    "    ef5s_diff = [ef - ef5s[0] for ef in ef5s]\n",
    "\n",
    "    # Create labels for the x-axis\n",
    "    x_labels = [str(t) if t is not None else \"No Cutoff\" for t in thresholds]\n",
    "\n",
    "    # Create a dataframe to hold the data\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Protein\": title_suffix,\n",
    "            \"Strain Energy Cutoff\": x_labels,\n",
    "            \"EF1%\": ef1s,\n",
    "            \"EF5%\": ef5s,\n",
    "            \"deltaEF1%\": ef1s_diff,\n",
    "            \"deltaEF5%\": ef5s_diff,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"Writing CSV to strain_enrichment_metrics_{title_suffix}.csv\")\n",
    "    df.to_csv(\n",
    "        f\"./papermill/csv/strain_enrichment_metrics_{title_suffix}.csv\",\n",
    "        index=False,\n",
    "    )\n",
    "    display(df)\n",
    "    print(\"CSV writing complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_enrichment_metrics(all_data, total_e_thresholds, title_suffix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Linear Log ROC AUC by Strain Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logauc_by_strain(df, a=1e-3, total_e_threshold=None):\n",
    "    # Filter dataframe based on 'Total_E' threshold if provided\n",
    "    if total_e_threshold is not None:\n",
    "        df = df[df[\"Total_E\"] <= total_e_threshold]\n",
    "\n",
    "    # Invert scores since lower scores indicate positive class\n",
    "    y_scores_inverted = -df[\"r_i_docking_score\"]\n",
    "\n",
    "    # Calculate FPR, TPR, and thresholds using sklearn\n",
    "    fpr, tpr, _ = roc_curve(df[\"Activity\"], y_scores_inverted)\n",
    "\n",
    "    # Select the thresholds that result in FPR >= a for log scale plotting\n",
    "    valid_indices = np.where(fpr >= a)\n",
    "    fpr_valid = fpr[valid_indices]\n",
    "    tpr_valid = tpr[valid_indices]\n",
    "\n",
    "    # Calculate log of FPR for valid indices\n",
    "    log_fpr_valid = np.log10(fpr_valid)\n",
    "\n",
    "    # Calculate the AUC for the valid range\n",
    "    linlog_auc = auc(log_fpr_valid, tpr_valid)\n",
    "\n",
    "    ### NOTE TIMES 10 NOTE ###\n",
    "    log_auc = (linlog_auc / -np.log10(a)) * 10\n",
    "\n",
    "    return log_auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_log_aucs(data, thresholds, title_suffix):\n",
    "    # Calculate log_auc for each threshold and plot\n",
    "    log_aucs = [logauc_by_strain(data, total_e_threshold=t) for t in thresholds]\n",
    "\n",
    "    # Create labels for the x-axis\n",
    "    x_labels = [str(t) if t is not None else \"No Cutoff\" for t in thresholds]\n",
    "\n",
    "    plt.bar(range(len(thresholds)), log_aucs, tick_label=x_labels)\n",
    "    plt.title(f\"Linear Log10 AUC by Strain Energy Cutoff ({title_suffix})\")\n",
    "    plt.xlabel(\"Strain Energy Cutoff\")\n",
    "    plt.ylabel(\"Linear Log10 AUC (x10)\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_log_aucs(all_data, total_e_thresholds, title_suffix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_delta_log_aucs(data, thresholds, title_suffix):\n",
    "    # Calculate log_auc for 'None' threshold\n",
    "    none_log_auc = logauc_by_strain(data, total_e_threshold=None)\n",
    "\n",
    "    # Calculate delta log_auc for each threshold and plot\n",
    "    delta_log_aucs = [\n",
    "        logauc_by_strain(data, total_e_threshold=t) - none_log_auc for t in thresholds\n",
    "    ]\n",
    "\n",
    "    # Create labels for the x-axis\n",
    "    x_labels = [str(t) if t is not None else \"No Cutoff\" for t in thresholds]\n",
    "\n",
    "    plt.bar(range(len(thresholds)), delta_log_aucs, tick_label=x_labels)\n",
    "    plt.title(f\"Delta Linear Log10 AUC by Strain Energy Cutoff ({title_suffix})\")\n",
    "    plt.xlabel(\"Strain Energy Cutoff\")\n",
    "    plt.ylabel(\"Delta Linear Log10 AUC (x10)\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_delta_log_aucs(all_data, total_e_thresholds, title_suffix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_log_aucs_delta_auc(data, thresholds, title_suffix):\n",
    "    # Calculate log_auc for each threshold\n",
    "    log_aucs = [logauc_by_strain(data, total_e_threshold=t) for t in thresholds]\n",
    "\n",
    "    delta_log_aucs = [0] + [log_aucs[i] - log_aucs[0] for i in range(1, len(log_aucs))]\n",
    "\n",
    "    # Create labels for the x-axis\n",
    "    x_labels = [str(t) if t is not None else \"No Cutoff\" for t in thresholds]\n",
    "\n",
    "    # Create a dataframe to hold the data\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Protein\": title_suffix,\n",
    "            \"Strain Energy Cutoff\": x_labels,\n",
    "            \"Linear Log10 AUC (x10)\": log_aucs,\n",
    "            \"Delta Linear Log10 AUC (x10)\": delta_log_aucs,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Write the dataframe to a CSV file\n",
    "    df.to_csv(f\"./papermill/csv/strain_log_aucs_{title_suffix}.csv\", index=False)\n",
    "    display(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_log_aucs_delta_auc(all_data, total_e_thresholds, title_suffix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_semi_log_roc(df, a=1e-3, total_e_threshold=None, ax=None, color=\"blue\"):\n",
    "    # Filter dataframe based on 'Total_E' threshold if provided\n",
    "    if total_e_threshold is not None:\n",
    "        df = df[df[\"Total_E\"] <= total_e_threshold]\n",
    "\n",
    "    # Invert scores since lower scores indicate positive class\n",
    "    y_scores_inverted = -df[\"r_i_docking_score\"]\n",
    "\n",
    "    # Calculate FPR, TPR, and thresholds using sklearn\n",
    "    fpr, tpr, _ = roc_curve(df[\"Activity\"], y_scores_inverted)\n",
    "\n",
    "    # Select the thresholds that result in FPR >= a for log scale plotting\n",
    "    valid_indices = np.where(fpr >= a)\n",
    "    fpr_valid = fpr[valid_indices]\n",
    "    tpr_valid = tpr[valid_indices]\n",
    "\n",
    "    # Calculate log of FPR for valid indices\n",
    "    log_fpr_valid = np.log10(fpr_valid)\n",
    "\n",
    "    # Calculate LogAUC\n",
    "    log_auc = (auc(log_fpr_valid, tpr_valid) / -np.log10(a)) * 10\n",
    "\n",
    "    # Count the number of rows with Activity = 1 and the total number of rows\n",
    "    activity_1_count = df[df[\"Activity\"] == 1].shape[0]\n",
    "    total_count = df.shape[0]\n",
    "\n",
    "    # Plot semi-log ROC curve\n",
    "    ax.plot(\n",
    "        log_fpr_valid,\n",
    "        tpr_valid,\n",
    "        label=f\"Threshold: {total_e_threshold if total_e_threshold is not None else 'N/A'}, LogAUC: {log_auc:.2f}, Actives: {activity_1_count}, Total count: {total_count}\",\n",
    "        color=color,\n",
    "    )\n",
    "    ax.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_thresholds(data, thresholds, title_suffix):\n",
    "    # Create a single plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "    # Define a colormap\n",
    "    cmap = cm.get_cmap(\n",
    "        \"viridis\", len(thresholds) - 1\n",
    "    )  # Subtract 1 because the first color is manually set\n",
    "\n",
    "    # Plot semi-log ROC curve for each threshold\n",
    "    for i, t in enumerate(thresholds):\n",
    "        # Filter dataframe based on 'Total_E' threshold\n",
    "        df_filtered = data if t is None else data[data[\"Total_E\"] <= t]\n",
    "\n",
    "        # Set a distinct color for the first threshold\n",
    "        color = (\n",
    "            \"red\" if t is None else cmap(i - 1)\n",
    "        )  # Subtract 1 because the first color is manually set\n",
    "\n",
    "        # Call the function with the filtered data\n",
    "        plot_semi_log_roc(df_filtered, total_e_threshold=t, ax=ax, color=color)\n",
    "\n",
    "    ax.set_title(f\"Strain Energy Thresholds ({title_suffix})\")\n",
    "    ax.set_xlabel(\"log(FPR)\")\n",
    "    ax.set_ylabel(\"TPR\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plot_all_thresholds(all_data, total_e_thresholds, title_suffix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # ROC AUC by Strain Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve_inverted(data, total_e_threshold, ax, title_suffix, color=\"blue\"):\n",
    "    \"\"\"\n",
    "    Plot the ROC curve for the given true labels and inverted scores.\n",
    "\n",
    "    :param data: The DataFrame containing the data.\n",
    "    :param total_e_threshold: The threshold for the 'Total_E' column. If this is not None, the data is filtered to only include rows where 'Total_E' is less than or equal to this threshold.\n",
    "    :param ax: The axes object to plot on.\n",
    "    :param title_suffix: The suffix to add to the title of the plot.\n",
    "    :param color: The color to use for the ROC curve.\n",
    "    \"\"\"\n",
    "    # Filter the data based on 'Total_E' threshold\n",
    "    df = (\n",
    "        data\n",
    "        if total_e_threshold is None\n",
    "        else data[data[\"Total_E\"] <= total_e_threshold]\n",
    "    )\n",
    "\n",
    "    # Get the true labels and scores\n",
    "    y_true = df[\"Activity\"]\n",
    "    y_scores = df[\"r_i_docking_score\"]\n",
    "\n",
    "    # Inverting the scores\n",
    "    y_scores_inverted = -y_scores\n",
    "\n",
    "    # Compute the ROC curve and AUC with inverted scores\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores_inverted)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Count the number of rows with Activity = 1 and the total number of rows\n",
    "    activity_1_count = df[df[\"Activity\"] == 1].shape[0]\n",
    "    total_count = df.shape[0]\n",
    "\n",
    "    # Plotting the ROC curve\n",
    "    # ax.plot(fpr, tpr, lw=2, label=f\"ROC curve (AUC = {roc_auc:.2f}), Actives: {activity_1_count}, Total Count: {total_count}\", color=color)\n",
    "\n",
    "    # Plotting the ROC curve\n",
    "    ax.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        lw=2,\n",
    "        label=f\"Threshold: {total_e_threshold if total_e_threshold is not None else 'N/A'}, ROC curve (AUC = {roc_auc:.2f}), Actives: {activity_1_count}, Total Count: {total_count}\",\n",
    "        color=color,\n",
    "    )\n",
    "\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.set_title(\n",
    "        f\"Receiver Operating Characteristic (Inverted Scores) ({title_suffix})\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curves_all_thresholds(data, thresholds, title_suffix):\n",
    "    # Create a single plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "    # Define a colormap\n",
    "    cmap = cm.get_cmap(\n",
    "        \"viridis\", len(thresholds) - 1\n",
    "    )  # Subtract 1 because the first color is manually set\n",
    "\n",
    "    # Plot ROC curve for each threshold\n",
    "    for i, t in enumerate(thresholds):\n",
    "        # Set a distinct color for the first threshold\n",
    "        color = (\n",
    "            \"red\" if t is None else cmap(i - 1)\n",
    "        )  # Subtract 1 because the first color is manually set\n",
    "\n",
    "        plot_roc_curve_inverted(\n",
    "            data, total_e_threshold=t, ax=ax, title_suffix=title_suffix, color=color\n",
    "        )\n",
    "\n",
    "    # Plot the random classifier line after all the ROC curves\n",
    "    ax.plot(\n",
    "        [0, 1], [0, 1], lw=2, linestyle=\"--\", label=\"Random Classifier\", color=\"grey\"\n",
    "    )\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curves_all_thresholds(all_data, total_e_thresholds, title_suffix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plot_difference_in_auc_by_strain(data, thresholds, title_suffix):\n",
    "    # Create a list to hold AUC values\n",
    "    auc_values = []\n",
    "\n",
    "    # Loop over each threshold\n",
    "    for t in thresholds:\n",
    "        # Filter the data based on 'Total_E' threshold\n",
    "        df = data if t is None else data[data[\"Total_E\"] <= t]\n",
    "\n",
    "        # Get the true labels and scores\n",
    "        y_true = df[\"Activity\"]\n",
    "        y_scores = df[\"r_i_docking_score\"]\n",
    "\n",
    "        # Inverting the scores\n",
    "        y_scores_inverted = -y_scores\n",
    "\n",
    "        # Compute the ROC curve and AUC with inverted scores\n",
    "        fpr, tpr, roc_thresholds = roc_curve(y_true, y_scores_inverted)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        # Add AUC to the list\n",
    "        auc_values.append(roc_auc)\n",
    "\n",
    "    # Calculate differences in AUC\n",
    "    auc_diff = [auc - auc_values[0] for auc in auc_values]\n",
    "\n",
    "    # Create labels for the x-axis\n",
    "    x_labels = [str(t) if t is not None else \"No Cutoff\" for t in thresholds]\n",
    "\n",
    "    # Create an array with the positions of each bar on the x axis\n",
    "    x = np.arange(len(x_labels))\n",
    "\n",
    "    # Set the width of the bars\n",
    "    bar_width = 0.35\n",
    "\n",
    "    plt.bar(x, auc_diff, bar_width, label=\"deltaAUC\")\n",
    "    plt.title(f\"Difference in AUC by Strain Energy Cutoff ({title_suffix})\")\n",
    "    plt.xlabel(\"Strain Energy Cutoff\")\n",
    "    plt.ylabel(\"Difference in AUC\")\n",
    "    plt.xticks(x, x_labels)  # Set the position and labels of the xticks\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_plot_difference_in_auc_by_strain(all_data, total_e_thresholds, title_suffix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_roc_metrics(data, thresholds, title_suffix):\n",
    "    # Create a list to hold the data\n",
    "    data_list = []\n",
    "\n",
    "    # List to hold AUC values\n",
    "    auc_values = []\n",
    "\n",
    "    # Loop over each threshold\n",
    "    for t in thresholds:\n",
    "        # Filter the data based on 'Total_E' threshold\n",
    "        df = data if t is None else data[data[\"Total_E\"] <= t]\n",
    "\n",
    "        # Get the true labels and scores\n",
    "        y_true = df[\"Activity\"]\n",
    "        y_scores = df[\"r_i_docking_score\"]\n",
    "\n",
    "        # Inverting the scores\n",
    "        y_scores_inverted = -y_scores\n",
    "\n",
    "        # Compute the ROC curve and AUC with inverted scores\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_scores_inverted)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        # Add AUC to the list\n",
    "        auc_values.append(roc_auc)\n",
    "\n",
    "        # Count the number of rows with Activity = 1 and the total number of rows\n",
    "        activity_1_count = df[df[\"Activity\"] == 1].shape[0]\n",
    "        total_count = df.shape[0]\n",
    "\n",
    "        # Add the data to the list\n",
    "        data_list.append(\n",
    "            {\n",
    "                \"Protein\": title_suffix,\n",
    "                \"Strain Energy Cutoff\": t if t is not None else \"No Cutoff\",\n",
    "                \"ROC_AUC\": roc_auc,\n",
    "                \"Actives\": activity_1_count,\n",
    "                \"Total Count\": total_count,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Calculate differences in AUC\n",
    "    auc_diff = [auc - auc_values[0] for auc in auc_values]\n",
    "\n",
    "    # Add deltaAUC to the data list\n",
    "    for i in range(len(data_list)):\n",
    "        data_list[i][\"deltaAUC\"] = auc_diff[i]\n",
    "\n",
    "    # Create a dataframe from the list\n",
    "    df = pd.DataFrame(data_list)\n",
    "\n",
    "    # Write the dataframe to a CSV file\n",
    "    df.to_csv(f\"./papermill/csv/strain_roc_metrics_{title_suffix}.csv\", index=False)\n",
    "    display(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_roc_metrics(all_data, total_e_thresholds, title_suffix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Pareto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = all_data\n",
    "chosen_rank_amount = 20\n",
    "rank_thresholds = [10, 20]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_pareto(scores):\n",
    "    population_size = scores.shape[0]\n",
    "    pareto_front = np.ones(population_size, dtype=bool)\n",
    "    for i in range(population_size):\n",
    "        for j in range(population_size):\n",
    "            if all(scores[j] <= scores[i]) and any(scores[j] < scores[i]):\n",
    "                pareto_front[i] = 0\n",
    "                break\n",
    "    return np.where(pareto_front == 1)[0]\n",
    "\n",
    "\n",
    "def find_pareto_ranks(scores, max_ranks=100):\n",
    "    ranks = []\n",
    "    remaining_scores = scores.copy()\n",
    "    remaining_indices = np.arange(scores.shape[0])\n",
    "    for _ in range(max_ranks):\n",
    "        pareto_indices = identify_pareto(remaining_scores)\n",
    "        ranks.append(remaining_indices[pareto_indices])\n",
    "        remaining_scores = np.delete(remaining_scores, pareto_indices, axis=0)\n",
    "        remaining_indices = np.delete(remaining_indices, pareto_indices)\n",
    "        if remaining_scores.shape[0] == 0:\n",
    "            break\n",
    "    return ranks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the scores for the entire dataset\n",
    "scores = data[[\"r_i_docking_score\", \"Total_E\"]].values\n",
    "\n",
    "# Find the `chosen_rank_amount` of Pareto fronts until all points are classified or a max limit is reached\n",
    "pareto_ranks_indices = find_pareto_ranks(\n",
    "    scores, max_ranks=chosen_rank_amount\n",
    ")  # Adjust max_ranks as needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the actual number of ranks found\n",
    "num_ranks = len(pareto_ranks_indices)\n",
    "\n",
    "# Plot the baseline data distribution\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(\n",
    "    data[\"r_i_docking_score\"],\n",
    "    data[\"Total_E\"],\n",
    "    color=\"lightgrey\",\n",
    "    label=\"Baseline Data\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "# Generate colors for each rank dynamically using a colormap\n",
    "colormap = viridis\n",
    "norm = Normalize(vmin=0, vmax=num_ranks - 1)\n",
    "\n",
    "for i, indices in enumerate(pareto_ranks_indices):\n",
    "    rank_data = data.iloc[indices]\n",
    "    plt.scatter(\n",
    "        rank_data[\"r_i_docking_score\"],\n",
    "        rank_data[\"Total_E\"],\n",
    "        color=colormap(norm(i)),\n",
    "        label=f\"Rank {i+1}\",\n",
    "    )\n",
    "\n",
    "# Create a custom legend\n",
    "sm = plt.cm.ScalarMappable(cmap=colormap, norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, ax=plt.gca(), ticks=np.linspace(0, num_ranks - 1, num_ranks))\n",
    "cbar.ax.set_yticklabels([f\"Rank {i+1}\" for i in range(num_ranks)])\n",
    "cbar.set_label(\"Pareto Front Rank\")\n",
    "\n",
    "plt.title(\n",
    "    f\"{chosen_rank_amount} Ranks of Pareto Fronts with {title_suffix} Baseline Data Distribution\"\n",
    ")\n",
    "plt.xlabel(\"r_i_docking_score\")\n",
    "plt.ylabel(\"Total_E\")\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of Pareto points per rank and the total\n",
    "num_points_per_rank = [len(indices) for indices in pareto_ranks_indices]\n",
    "total_points = sum(num_points_per_rank)\n",
    "\n",
    "# Print the counts\n",
    "print(\"Number of Pareto points per rank:\")\n",
    "for rank, count in enumerate(num_points_per_rank, start=1):\n",
    "    print(f\"Rank {rank}: {count} points\")\n",
    "\n",
    "print(f\"\\nTotal Pareto points across all ranks: {total_points}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pareto_ranks_indices = np.concatenate(pareto_ranks_indices)\n",
    "# display(data.iloc[all_pareto_ranks_indices])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_front_df = data.iloc[all_pareto_ranks_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_density_docking(pareto_front_df, title_suffix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_density_strain(pareto_front_df, title_suffix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pareto_front_df[\"r_i_docking_score\"], pareto_front_df[\"Total_E\"])\n",
    "actives_pareto_front_df = pareto_front_df[pareto_front_df[\"Activity\"] == 1]\n",
    "plt.scatter(\n",
    "    actives_pareto_front_df[\"r_i_docking_score\"], actives_pareto_front_df[\"Total_E\"]\n",
    ")\n",
    "plt.title(f\"r_i_docking_score vs. Total_E ({title_suffix})\")\n",
    "plt.xlabel(\"r_i_docking_score\")\n",
    "plt.ylabel(\"Total_E\")\n",
    "plt.legend([\"Decoys\", \"Actives\"])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Pareto Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_front_df = calculate_enrichment_parameters(pareto_front_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrichment_metrics(df):\n",
    "    df = calculate_enrichment_parameters(df)\n",
    "    closest_to_one_percent = df.iloc[\n",
    "        (df[\"Percentage_Screened\"] - 0.01).abs().argsort()[:1]\n",
    "    ]\n",
    "    ef1 = (\n",
    "        closest_to_one_percent[\"Cumulative_Actives\"].values[0]\n",
    "        / closest_to_one_percent[\"Total_Actives\"].values[0]\n",
    "        * 100\n",
    "    )\n",
    "    closest_to_five_percent = df.iloc[\n",
    "        (df[\"Percentage_Screened\"] - 0.05).abs().argsort()[:1]\n",
    "    ]\n",
    "    ef5 = (\n",
    "        closest_to_five_percent[\"Cumulative_Actives\"].values[0]\n",
    "        / closest_to_five_percent[\"Total_Actives\"].values[0]\n",
    "        * 100\n",
    "    )\n",
    "    return ef1, ef5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_enrichment_metrics_for_ranks(\n",
    "    df, pareto_ranks_indices, rank_thresholds, title_suffix=title_suffix\n",
    "):\n",
    "    # Calculate enrichment for the entire dataset\n",
    "    ef1_full, ef5_full = enrichment_metrics(df)\n",
    "\n",
    "    # Prepare plot\n",
    "    labels = [\"Baseline\"] + [f\"Top {rank} Ranks\" for rank in rank_thresholds]\n",
    "    ef1_values = [ef1_full]\n",
    "    ef5_values = [ef5_full]\n",
    "\n",
    "    # Calculate and append metrics for each specified rank threshold\n",
    "    for rank_threshold in rank_thresholds:\n",
    "        top_ranks_indices = np.concatenate(pareto_ranks_indices[:rank_threshold])\n",
    "        subset_data = df.iloc[top_ranks_indices]\n",
    "        ef1_subset, ef5_subset = enrichment_metrics(subset_data)\n",
    "        ef1_values.append(ef1_subset)\n",
    "        ef5_values.append(ef5_subset)\n",
    "\n",
    "    # Plotting\n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "    width = 0.35  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots(\n",
    "        # figsize=(10, 6)\n",
    "    )\n",
    "    rects1 = ax.bar(x - width / 2, ef1_values, width, label=\"EF1%\")\n",
    "    rects2 = ax.bar(x + width / 2, ef5_values, width, label=\"EF5%\")\n",
    "\n",
    "    ax.set_ylabel(\"Enrichment Factor (%)\")\n",
    "    ax.set_title(\n",
    "        f\"Enrichment Factors Comparison by Pareto Rank Threshold {title_suffix}\"\n",
    "    )\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "compare_enrichment_metrics_for_ranks(\n",
    "    data, pareto_ranks_indices, rank_thresholds, title_suffix=title_suffix\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plot_delta_enrichment_by_pareto(\n",
    "    df, pareto_ranks_indices, rank_thresholds, title_suffix\n",
    "):\n",
    "    # Calculate enrichment for the entire dataset as baseline\n",
    "    ef1_full, ef5_full = enrichment_metrics(df)\n",
    "\n",
    "    # Initialize lists to store delta values\n",
    "    delta_ef1s = []\n",
    "    delta_ef5s = []\n",
    "\n",
    "    # Calculate metrics for each specified rank threshold and compute deltas\n",
    "    for rank_threshold in rank_thresholds:\n",
    "        top_ranks_indices = np.concatenate(pareto_ranks_indices[:rank_threshold])\n",
    "        subset_data = df.iloc[top_ranks_indices]\n",
    "        ef1_subset, ef5_subset = enrichment_metrics(subset_data)\n",
    "\n",
    "        # Calculate deltas compared to baseline\n",
    "        delta_ef1s.append(ef1_subset - ef1_full)\n",
    "        delta_ef5s.append(ef5_subset - ef5_full)\n",
    "\n",
    "    # Create labels for the x-axis\n",
    "    labels = [f\"Top {rank} Ranks\" for rank in rank_thresholds]\n",
    "\n",
    "    # Create an array with the positions of each bar on the x axis\n",
    "    x = np.arange(len(labels))\n",
    "\n",
    "    # Set the width of the bars\n",
    "    bar_width = 0.35\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure()\n",
    "    plt.bar(x - bar_width / 2, delta_ef1s, bar_width, label=\"EF1%\")\n",
    "    plt.bar(x + bar_width / 2, delta_ef5s, bar_width, label=\"EF5%\")\n",
    "\n",
    "    plt.title(f\"Delta Enrichment Factors by Pareto Rank Threshold ({title_suffix})\")\n",
    "    plt.xlabel(\"Pareto Rank Threshold\")\n",
    "    plt.ylabel(\"Delta Enrichment Factor (%)\")\n",
    "    plt.xticks(x, labels, rotation=45, ha=\"right\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example, assuming `data`, `pareto_ranks_indices`, and `title_suffix` are defined\n",
    "bar_plot_delta_enrichment_by_pareto(\n",
    "    data, pareto_ranks_indices, rank_thresholds, title_suffix=title_suffix\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_enrichment_curve_by_pareto(\n",
    "    df, pareto_indices, ax, label_prefix=\"\", color=\"red\"\n",
    "):\n",
    "    df = calculate_enrichment_parameters(df)\n",
    "    ef1, ef5 = enrichment_metrics(df)\n",
    "    enrichment_auc = auc(df[\"Percentage_Screened\"], df[\"Fraction_Actives\"])\n",
    "\n",
    "    # Plot the enrichment curve\n",
    "    ax.plot(\n",
    "        df[\"Percentage_Screened\"] * 100,\n",
    "        df[\"Fraction_Actives\"] * 100,\n",
    "        label=f\"{label_prefix}\\nEnrichment AUC = {enrichment_auc:.2f}\\n(EF1% = {ef1:.1f}%)\\n(EF5% = {ef5:.0f}%)\",\n",
    "        color=color,\n",
    "    )\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "def plot_enrichment_curves_by_pareto_ranks(\n",
    "    data, pareto_ranks_indices, rank_thresholds, title_suffix\n",
    "):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Plot for the entire dataset as a baseline\n",
    "    plot_enrichment_curve_by_pareto(\n",
    "        data, np.arange(len(data)), ax, label_prefix=\"Baseline\", color=\"red\"\n",
    "    )\n",
    "\n",
    "    # Define a colormap\n",
    "    cmap = plt.get_cmap(\"viridis\", len(rank_thresholds))\n",
    "\n",
    "    # Plot enrichment curves for specified Pareto rank thresholds\n",
    "    for i, rank_threshold in enumerate(rank_thresholds):\n",
    "        top_ranks_indices = np.concatenate(pareto_ranks_indices[:rank_threshold])\n",
    "        subset_data = data.iloc[top_ranks_indices]\n",
    "        plot_enrichment_curve_by_pareto(\n",
    "            subset_data,\n",
    "            top_ranks_indices,\n",
    "            ax,\n",
    "            label_prefix=f\"Top {rank_threshold} Ranks\",\n",
    "            color=cmap(i),\n",
    "        )\n",
    "\n",
    "    ax.set_title(f\"Enrichment Curves by Pareto Rank Thresholds ({title_suffix})\")\n",
    "    ax.set_xlabel(\"Percent Screened (%)\")\n",
    "    ax.set_ylabel(\"Percent Identified Actives (%)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_enrichment_curves_by_pareto_ranks(\n",
    "    data, pareto_ranks_indices, rank_thresholds, title_suffix\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_enrichment_metrics_pareto(\n",
    "    df, pareto_ranks_indices, rank_thresholds, title_suffix\n",
    "):\n",
    "    # Calculate enrichment for the entire dataset as baseline\n",
    "    ef1_full, ef5_full = enrichment_metrics(df)\n",
    "\n",
    "    # Initalize lists to store ef1 and ef5 values\n",
    "    ef1_values = [ef1_full]\n",
    "    ef5_values = [ef5_full]\n",
    "\n",
    "    # Initialize lists to store delta values\n",
    "    delta_ef1s = [0]  # The difference in EF1% for the baseline is 0\n",
    "    delta_ef5s = [0]  # The difference in EF5% for the baseline is 0\n",
    "\n",
    "    # Calculate metrics for each specified rank threshold and compute deltas\n",
    "    for rank_threshold in rank_thresholds:\n",
    "        top_ranks_indices = np.concatenate(pareto_ranks_indices[:rank_threshold])\n",
    "        subset_data = df.iloc[top_ranks_indices]\n",
    "        ef1_subset, ef5_subset = enrichment_metrics(subset_data)\n",
    "\n",
    "        # Append metrics to lists\n",
    "        ef1_values.append(ef1_subset)\n",
    "        ef5_values.append(ef5_subset)\n",
    "\n",
    "        # Calculate deltas compared to baseline\n",
    "        delta_ef1s.append(ef1_subset - ef1_full)\n",
    "        delta_ef5s.append(ef5_subset - ef5_full)\n",
    "\n",
    "    # Create labels for the x-axis\n",
    "    labels = [\"No Cutoff\"] + [f\"Top {rank} Pareto Ranks\" for rank in rank_thresholds]\n",
    "\n",
    "    # Create a DataFrame with the data\n",
    "    data = {\n",
    "        \"Protein\": title_suffix,\n",
    "        \"Strain Energy Cutoff\": labels,  # mimic prior data column header\n",
    "        \"EF1%\": ef1_values,\n",
    "        \"EF5%\": ef5_values,\n",
    "        \"deltaEF1%\": delta_ef1s,\n",
    "        \"deltaEF5%\": delta_ef5s,\n",
    "    }\n",
    "    df_to_csv = pd.DataFrame(data)\n",
    "\n",
    "    # Write the DataFrame to a CSV file\n",
    "    df_to_csv.to_csv(\n",
    "        f\"./papermill/csv/strain_enrichment_metrics_pareto_{title_suffix}.csv\",\n",
    "        index=False,\n",
    "    )\n",
    "    display(df_to_csv)\n",
    "\n",
    "\n",
    "write_enrichment_metrics_pareto(\n",
    "    data, pareto_ranks_indices, [10, 20], title_suffix=title_suffix\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Pareto Linear Log ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logauc_by_pareto(df, pareto_ranks_indices, rank_thresholds, a=1e-3):\n",
    "    # Filter the data based on the Pareto rank threshold, if provided\n",
    "    if pareto_ranks_indices is not None:\n",
    "        df = df.iloc[np.concatenate(pareto_ranks_indices[:rank_thresholds])]\n",
    "\n",
    "    print(df.shape)\n",
    "\n",
    "    y_scores_inverted = -df[\"r_i_docking_score\"]\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(df[\"Activity\"], y_scores_inverted)\n",
    "\n",
    "    valid_indices = np.where(fpr >= a)\n",
    "    fpr_valid = fpr[valid_indices]\n",
    "    tpr_valid = tpr[valid_indices]\n",
    "\n",
    "    log_fpr_valid = np.log10(fpr_valid)\n",
    "\n",
    "    linlog_auc = auc(log_fpr_valid, tpr_valid)\n",
    "\n",
    "    ### NOTE TIMES 10 NOTE ###\n",
    "    log_auc = (linlog_auc / -np.log10(a)) * 10\n",
    "\n",
    "    return log_auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_log_aucs_pareto(data, pareto_ranks_indices, rank_thresholds, title_suffix):\n",
    "    # Calculate log AUC for the entire dataset\n",
    "    log_auc_all = logauc_by_pareto(data, None, None)\n",
    "\n",
    "    # Calculate log AUC for each rank threshold\n",
    "    log_aucs = [log_auc_all] + [\n",
    "        logauc_by_pareto(data, pareto_ranks_indices, rank_threshold)\n",
    "        for rank_threshold in rank_thresholds\n",
    "    ]\n",
    "\n",
    "    # Create labels for \"All Data\" and each rank threshold\n",
    "    labels = [\"Baseline\"] + [f\"Top {rank} Ranks\" for rank in rank_thresholds]\n",
    "\n",
    "    plt.bar(range(len(labels)), log_aucs, tick_label=labels)\n",
    "    plt.title(f\"Linear Log10 AUC by Pareto Rank Thresholds ({title_suffix})\")\n",
    "    plt.xlabel(\"Pareto Rank Threshold\")\n",
    "    plt.ylabel(\"Linear Log10 AUC (x10)\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_log_aucs_pareto(data, pareto_ranks_indices, rank_thresholds, title_suffix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_delta_log_aucs_pareto(\n",
    "    data, pareto_ranks_indices, rank_thresholds, title_suffix\n",
    "):\n",
    "    # Calculate log AUC for the entire dataset\n",
    "    log_auc_all = logauc_by_pareto(data, None, None)\n",
    "\n",
    "    # Calculate delta log AUC for each rank threshold\n",
    "    delta_log_aucs = [log_auc_all - log_auc_all] + [\n",
    "        logauc_by_pareto(data, pareto_ranks_indices, rank_threshold) - log_auc_all\n",
    "        for rank_threshold in rank_thresholds\n",
    "    ]\n",
    "\n",
    "    # Create labels for \"All Data\" and each rank threshold\n",
    "    labels = [\"Baseline\"] + [f\"Top {rank} Ranks\" for rank in rank_thresholds]\n",
    "\n",
    "    plt.bar(range(len(labels)), delta_log_aucs, tick_label=labels)\n",
    "    plt.title(f\"Delta Linear Log10 AUC by Pareto Rank Thresholds ({title_suffix})\")\n",
    "    plt.xlabel(\"Pareto Rank Threshold\")\n",
    "    plt.ylabel(\"Delta Linear Log10 AUC (x10)\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_delta_log_aucs_pareto(data, pareto_ranks_indices, rank_thresholds, title_suffix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_semi_log_roc_pareto(\n",
    "    df, a=1e-3, pareto_ranks_indices=None, rank_threshold=None, ax=None, color=\"blue\"\n",
    "):\n",
    "    # Filter the data based on the Pareto rank threshold, if provided\n",
    "    if pareto_ranks_indices is not None:\n",
    "        df = df.iloc[np.concatenate(pareto_ranks_indices[:rank_threshold])]\n",
    "\n",
    "    y_scores_inverted = -df[\"r_i_docking_score\"]\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(df[\"Activity\"], y_scores_inverted)\n",
    "\n",
    "    valid_indices = np.where(fpr >= a)\n",
    "    fpr_valid = fpr[valid_indices]\n",
    "    tpr_valid = tpr[valid_indices]\n",
    "\n",
    "    log_fpr_valid = np.log10(fpr_valid)\n",
    "\n",
    "    linlog_auc = auc(log_fpr_valid, tpr_valid) / -np.log10(a)\n",
    "\n",
    "    activity_1_count = df[df[\"Activity\"] == 1].shape[0]\n",
    "    total_count = df.shape[0]\n",
    "\n",
    "    ax.plot(\n",
    "        log_fpr_valid,\n",
    "        tpr_valid,\n",
    "        label=f\"Pareto Rank: {rank_threshold if rank_threshold is not None else 'N/A'}, LogAUC: {linlog_auc:.2f}, Actives: {activity_1_count}, Total count: {total_count}\",\n",
    "        color=color,\n",
    "    )\n",
    "    ax.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_thresholds_pareto(df, pareto_ranks_indices, rank_thresholds, title_suffix):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "    plot_semi_log_roc_pareto(\n",
    "        df, a=1e-3, pareto_ranks_indices=None, rank_threshold=None, ax=ax, color=\"red\"\n",
    "    )\n",
    "\n",
    "    cmap = cm.get_cmap(\"viridis\", len(rank_thresholds) + 1)\n",
    "\n",
    "    for i, rank_threshold in enumerate(rank_thresholds):\n",
    "        plot_semi_log_roc_pareto(\n",
    "            df,\n",
    "            a=1e-3,\n",
    "            pareto_ranks_indices=pareto_ranks_indices,\n",
    "            rank_threshold=rank_threshold,\n",
    "            ax=ax,\n",
    "            color=cmap(i),\n",
    "        )\n",
    "\n",
    "    ax.set_title(f\"Pareto Rank Thresholds ({title_suffix})\")\n",
    "    ax.set_xlabel(\"log(FPR)\")\n",
    "    ax.set_ylabel(\"TPR\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_thresholds_pareto(data, pareto_ranks_indices, rank_thresholds, title_suffix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_log_aucs_pareto(data, pareto_ranks_indices, rank_thresholds, title_suffix):\n",
    "    # Calculate log AUC for the entire dataset\n",
    "    log_auc_all = logauc_by_pareto(data, None, None)\n",
    "\n",
    "    # Calculate log AUC for each rank threshold\n",
    "    log_aucs = [log_auc_all] + [\n",
    "        logauc_by_pareto(data, pareto_ranks_indices, rank_threshold)\n",
    "        for rank_threshold in rank_thresholds\n",
    "    ]\n",
    "\n",
    "    # Calculate delta log AUC for each rank threshold\n",
    "    delta_log_aucs = [log_auc_all - log_auc_all] + [\n",
    "        logauc_by_pareto(data, pareto_ranks_indices, rank_threshold) - log_auc_all\n",
    "        for rank_threshold in rank_thresholds\n",
    "    ]\n",
    "\n",
    "    # Create labels for \"All Data\" and each rank threshold\n",
    "    labels = [\"No Cutoff\"] + [f\"Top {rank} Pareto Ranks\" for rank in rank_thresholds]\n",
    "\n",
    "    # Create a dataframe to hold the data\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Protein\": title_suffix,\n",
    "            \"Strain Energy Cutoff\": labels,\n",
    "            \"Linear Log10 AUC (x10)\": log_aucs,\n",
    "            \"Delta Linear Log10 AUC (x10)\": delta_log_aucs,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Write the dataframe to a CSV file\n",
    "    df.to_csv(f\"./papermill/csv/strain_log_aucs_pareto_{title_suffix}.csv\", index=False)\n",
    "    display(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_log_aucs_pareto(data, pareto_ranks_indices, rank_thresholds, title_suffix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Pareto ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve_inverted_pareto(\n",
    "    data, pareto_ranks_indices, rank_threshold, ax, title_suffix, color=\"blue\"\n",
    "):\n",
    "    # Filter the data based on 'Total_E' threshold\n",
    "    if pareto_ranks_indices is None:\n",
    "        df = data\n",
    "    else:\n",
    "        df = data.iloc[np.concatenate(pareto_ranks_indices[:rank_threshold])]\n",
    "\n",
    "    # Get the true labels and scores\n",
    "    y_true = df[\"Activity\"]\n",
    "    y_scores = df[\"r_i_docking_score\"]\n",
    "\n",
    "    # Inverting the scores\n",
    "    y_scores_inverted = -y_scores\n",
    "\n",
    "    # Compute the ROC curve and AUC with inverted scores\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores_inverted)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Count the number of rows with Activity = 1 and the total number of rows\n",
    "    activity_1_count = df[df[\"Activity\"] == 1].shape[0]\n",
    "    total_count = df.shape[0]\n",
    "\n",
    "    # Plotting the ROC curve\n",
    "    ax.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        lw=2,\n",
    "        label=f\"Pareto Rank: {rank_threshold if rank_thresholds is not None else 'N/A'}, ROC curve (AUC = {roc_auc:.2f}), Actives: {activity_1_count}, Total Count: {total_count}\",\n",
    "        color=color,\n",
    "    )\n",
    "\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.set_title(\n",
    "        f\"Receiver Operating Characteristic (Inverted Scores) ({title_suffix})\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_roc_thresholds_pareto(\n",
    "    df, pareto_ranks_indices, rank_thresholds, title_suffix\n",
    "):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "    plot_roc_curve_inverted_pareto(\n",
    "        df,\n",
    "        pareto_ranks_indices=None,\n",
    "        rank_threshold=None,\n",
    "        ax=ax,\n",
    "        color=\"red\",\n",
    "        title_suffix=title_suffix,\n",
    "    )\n",
    "\n",
    "    cmap = cm.get_cmap(\"viridis\", len(rank_thresholds) + 1)\n",
    "\n",
    "    for i, rank_threshold in enumerate(rank_thresholds):\n",
    "        plot_roc_curve_inverted_pareto(\n",
    "            df,\n",
    "            pareto_ranks_indices=pareto_ranks_indices,\n",
    "            rank_threshold=rank_threshold,\n",
    "            ax=ax,\n",
    "            color=cmap(i),\n",
    "            title_suffix=title_suffix,\n",
    "        )\n",
    "\n",
    "    ax.set_title(f\"Pareto Rank Thresholds ({title_suffix})\")\n",
    "    ax.set_xlabel(\"FPR\")\n",
    "    ax.set_ylabel(\"TPR\")\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_roc_thresholds_pareto(\n",
    "    df=data,\n",
    "    pareto_ranks_indices=pareto_ranks_indices,\n",
    "    rank_thresholds=rank_thresholds,\n",
    "    title_suffix=title_suffix,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plot_difference_in_auc_by_pareto(\n",
    "    data, pareto_ranks_indices, rank_thresholds, title_suffix\n",
    "):\n",
    "    # Get the true labels and scores for the entire dataset\n",
    "    y_true = data[\"Activity\"]\n",
    "    y_scores = data[\"r_i_docking_score\"]\n",
    "\n",
    "    # Inverting the scores\n",
    "    y_scores_inverted = -y_scores\n",
    "\n",
    "    # Compute the ROC curve and AUC with inverted scores for the entire dataset\n",
    "    fpr, tpr, roc_thresholds = roc_curve(y_true, y_scores_inverted)\n",
    "    baseline_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Create a list to hold AUC values\n",
    "    auc_values = []\n",
    "\n",
    "    # Add the baseline AUC to the list of AUC values\n",
    "    auc_values.append(baseline_auc)\n",
    "\n",
    "    # Loop over each rank threshold\n",
    "    for rank_threshold in rank_thresholds:\n",
    "        # Filter the data based on Pareto rank threshold\n",
    "        df = (\n",
    "            data\n",
    "            if rank_threshold is None\n",
    "            else data.iloc[np.concatenate(pareto_ranks_indices[:rank_threshold])]\n",
    "        )\n",
    "\n",
    "        # Get the true labels and scores\n",
    "        y_true = df[\"Activity\"]\n",
    "        y_scores = df[\"r_i_docking_score\"]\n",
    "\n",
    "        # Inverting the scores\n",
    "        y_scores_inverted = -y_scores\n",
    "\n",
    "        # Compute the ROC curve and AUC with inverted scores\n",
    "        fpr, tpr, roc_thresholds = roc_curve(y_true, y_scores_inverted)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        # Add AUC to the list\n",
    "        auc_values.append(roc_auc)\n",
    "\n",
    "    # Calculate differences in AUC against the baseline\n",
    "    auc_diff = [auc - baseline_auc for auc in auc_values]\n",
    "\n",
    "    # Create labels for the x-axis\n",
    "    x_labels = [\"Baseline\"] + [\n",
    "        str(t) if t is not None else \"No Cutoff\" for t in rank_thresholds\n",
    "    ]\n",
    "\n",
    "    # Create an array with the positions of each bar on the x axis\n",
    "    x = np.arange(len(x_labels))\n",
    "\n",
    "    # Set the width of the bars\n",
    "    bar_width = 0.35\n",
    "\n",
    "    plt.bar(x, auc_diff, bar_width, label=\"deltaAUC\")\n",
    "    plt.title(f\"Difference in AUC by Pareto Rank Threshold ({title_suffix})\")\n",
    "    plt.xlabel(\"Pareto Rank Threshold\")\n",
    "    plt.ylabel(\"Difference in AUC\")\n",
    "    plt.xticks(x, x_labels)  # Set the position and labels of the xticks\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_plot_difference_in_auc_by_pareto(\n",
    "    data, pareto_ranks_indices, rank_thresholds, title_suffix\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pareto_roc_metrics(data, pareto_ranks_indices, rank_thresholds, title_suffix):\n",
    "    # Create a list to hold the data\n",
    "    data_list = []\n",
    "\n",
    "    # Get the true labels and scores for the entire dataset\n",
    "    y_true = data[\"Activity\"]\n",
    "    y_scores = data[\"r_i_docking_score\"]\n",
    "\n",
    "    # Inverting the scores\n",
    "    y_scores_inverted = -y_scores\n",
    "\n",
    "    # Compute the ROC curve and AUC with inverted scores for the entire dataset\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores_inverted)\n",
    "    baseline_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Count the number of rows with Activity = 1 and the total number of rows for the entire dataset\n",
    "    activity_1_count = data[data[\"Activity\"] == 1].shape[0]\n",
    "    total_count = data.shape[0]\n",
    "\n",
    "    # Add the baseline data to the list\n",
    "    data_list.append(\n",
    "        {\n",
    "            \"Protein\": title_suffix,\n",
    "            \"Strain Energy Cutoff\": \"No Cutoff\",\n",
    "            \"ROC_AUC\": baseline_auc,\n",
    "            \"Actives\": activity_1_count,\n",
    "            \"Total Count\": total_count,\n",
    "            \"deltaAUC\": 0,  # The difference in AUC for the baseline is 0\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Loop over each rank threshold\n",
    "    for rank_threshold in rank_thresholds:\n",
    "        # Filter the data based on Pareto rank threshold\n",
    "        df = data.iloc[np.concatenate(pareto_ranks_indices[:rank_threshold])]\n",
    "\n",
    "        # Get the true labels and scores\n",
    "        y_true = df[\"Activity\"]\n",
    "        y_scores = df[\"r_i_docking_score\"]\n",
    "\n",
    "        # Inverting the scores\n",
    "        y_scores_inverted = -y_scores\n",
    "\n",
    "        # Compute the ROC curve and AUC with inverted scores\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_scores_inverted)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        # Count the number of rows with Activity = 1 and the total number of rows\n",
    "        activity_1_count = df[df[\"Activity\"] == 1].shape[0]\n",
    "        total_count = df.shape[0]\n",
    "\n",
    "        # Add the data to the list\n",
    "        data_list.append(\n",
    "            {\n",
    "                \"Protein\": title_suffix,\n",
    "                \"Strain Energy Cutoff\": \"Top {} Pareto Ranks\".format(rank_threshold),\n",
    "                \"ROC_AUC\": roc_auc,\n",
    "                \"Actives\": activity_1_count,\n",
    "                \"Total Count\": total_count,\n",
    "                \"deltaAUC\": roc_auc\n",
    "                - baseline_auc,  # Calculate the difference in AUC against the baseline\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Create a dataframe from the list\n",
    "    df = pd.DataFrame(data_list)\n",
    "\n",
    "    # Write the dataframe to a CSV file\n",
    "    df.to_csv(\n",
    "        f\"./papermill/csv/strain_roc_metrics_pareto_{title_suffix}.csv\", index=False\n",
    "    )\n",
    "    display(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_pareto_roc_metrics(data, pareto_ranks_indices, rank_thresholds, title_suffix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Pareto Ranks as Scores\n",
    "\n",
    "  1. Calculate Pareto ranks of an the entire dataset\n",
    "\n",
    "  2. For Enrichment: Rank order by pareto rank (however that is an awkward implementation, as large sets of compounds will share a rank)\n",
    "\n",
    "  3. for ROC-AUC and ROC-logAUC, pareto ranking becomes the scoring function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  We need a new find_pareto_ranks() that does not have a limit on the amount of ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_pareto_ranks(scores):\n",
    "    ranks = []\n",
    "    remaining_scores = scores.copy()\n",
    "    remaining_indices = np.arange(scores.shape[0])\n",
    "    while remaining_scores.shape[0] > 0:\n",
    "        pareto_indices = identify_pareto(remaining_scores)\n",
    "        ranks.append(remaining_indices[pareto_indices])\n",
    "        remaining_scores = np.delete(remaining_scores, pareto_indices, axis=0)\n",
    "        remaining_indices = np.delete(remaining_indices, pareto_indices)\n",
    "    return ranks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Due to the length of time to calculate all the pareto fronts and their ranks, we are temporarily saving that data to file. This needs to be removed in papermill execution or we will get incorrect results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the scores for the entire dataset\n",
    "\n",
    "scores = data[[\"r_i_docking_score\", \"Total_E\"]].values\n",
    "\n",
    "# ! TEMPORARY COMMENTED OUT FOR TESTING\n",
    "total_pareto_ranks_indices = find_all_pareto_ranks(scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# ! TEMPORARY COMMENTED OUT FOR TESTING\n",
    "# ! FILE IS SAVED\n",
    "# with open('pareto_ranks.pkl', 'wb') as f:\n",
    "#   pickle.dump(total_pareto_ranks_indices, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('pareto_ranks.pkl', 'rb') as f:\n",
    "#   total_pareto_ranks_indices = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pareto_ranks_indices[:][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of Pareto points per rank and the total\n",
    "num_points_per_rank = [len(indices) for indices in total_pareto_ranks_indices]\n",
    "total_points = sum(num_points_per_rank)\n",
    "\n",
    "# Print the counts\n",
    "print(\"Number of Pareto points per rank:\")\n",
    "for rank, count in enumerate(num_points_per_rank, start=1):\n",
    "    print(f\"Rank {rank}: {count} points\")\n",
    "\n",
    "print(f\"\\nTotal Pareto points across all ranks: {total_points}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the actual number of ranks found\n",
    "num_ranks = len(total_pareto_ranks_indices)\n",
    "print(\n",
    "    f\"Number of Pareto ranks found: {num_ranks}, in total containing {total_points} points, original data is {data.shape}.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new figure and axes\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Normalize color gradient based on num_ranks, be careful how it is defined in the code\n",
    "norm = plt.cm.colors.Normalize(vmin=0, vmax=num_ranks - 1)\n",
    "\n",
    "# Set the title\n",
    "ax.set_title(f\"All Pareto Ranks ({title_suffix})\")\n",
    "\n",
    "# Use the axes for the scatter plots\n",
    "ax.scatter(\n",
    "    data[\"r_i_docking_score\"],\n",
    "    data[\"Total_E\"],\n",
    "    color=\"lightgrey\",\n",
    "    label=\"Baseline Data\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "for i, indices in enumerate(total_pareto_ranks_indices):\n",
    "    rank_data = data.iloc[indices]\n",
    "    ax.scatter(\n",
    "        rank_data[\"r_i_docking_score\"],\n",
    "        rank_data[\"Total_E\"],\n",
    "        color=colormap(norm(i)),\n",
    "        label=f\"Rank {i+1}\",\n",
    "    )\n",
    "\n",
    "# Use the axes for the colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap=colormap, norm=norm)\n",
    "sm.set_array([])\n",
    "# Reduce the number of ticks on the colorbar\n",
    "cbar = fig.colorbar(sm, ax=ax, ticks=np.linspace(0, num_ranks - 1, num_ranks // 5))\n",
    "cbar.ax.set_yticklabels([f\"Rank {i*5+1}\" for i in range(num_ranks // 5)])\n",
    "cbar.set_label(\"Pareto Front Rank\")\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pareto_indices = np.concatenate(total_pareto_ranks_indices)\n",
    "# Remember \".iloc\" is indices based, even the df.index has both indices and labels\n",
    "all_pareto_front_df = data.iloc[all_pareto_indices]\n",
    "display(all_pareto_front_df.head())\n",
    "display(data.head())\n",
    "display(total_pareto_ranks_indices[:][0])\n",
    "display(all_pareto_indices[:4])\n",
    "display(all_pareto_front_df.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(all_pareto_front_df.loc[1].to_frame().T)\n",
    "# display(all_pareto_front_df.iloc[0].to_frame().T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  `all_pareto_front_df` is in the correct \"order\" of pareto fronts by rank, however that is not very useful directly. as points in a front are considered \"optimal\". while I didn't intentionall set it: the ranks themselves are ordered by docking score **within** the rank itself. this could be useful later, so I don't want to disregard it.\n",
    "\n",
    "  however, what we need right now is the ability to map `total_pareto_ranks_indices` to the `all_pareto_front_df` dataframe. `total_pareto_ranks_indices` is a list of arrays containing the pareto front points, by their rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_pareto_ranks_indices[0])\n",
    "len(total_pareto_ranks_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rank, indices in enumerate(total_pareto_ranks_indices):\n",
    "    print(f\"Rank {rank+1}: {indices} points\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Because of how we handled the enrichment calculations (setting index values to start at 1 and not 0), we now need a work around for using iloc. I don't really get why, but I can't get a solution other than the strange one below.\n",
    "\n",
    "  iloc is indices based, **this is different than saying index based**, so iloc[0] is the first row of your dataframe. when we set our **labels** to start at 1, the index value we see as iloc[0] is 1\n",
    "\n",
    "  total_pareto_rank_indices has the actual **indicies** of the dataframe, which require using iloc to access successfully, unless we want to redo the index labels. i am too worried right now that this will introduce new problems somehow, so i don't want to touch it.\n",
    "\n",
    "  instead, we need to use get_loc. this is very unintuitive because you would think that get_loc (a pandas function) would return something you would use for df.loc related tasks. but no, get_loc is a method of the index class and is used to get the indice for a labeled index value in the dataframe.\n",
    "\n",
    "  get_loc() here is actually returning just one number, that is whatever the column based index that ParetoRank is.\n",
    "\n",
    "  also, i should have really handled my version control differently. this should have been a branch so i could have definitely kept the working completely okay code. i should have also used git tag to confirm that the code works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original dataframe, data\n",
    "df_copy = data.copy()\n",
    "\n",
    "# Create a new column 'ParetoRank' with default value NaN\n",
    "df_copy[\"ParetoRank\"] = np.nan\n",
    "\n",
    "# Get the integer position of the 'ParetoRank' column\n",
    "pareto_rank_col_index = df_copy.columns.get_loc(\"ParetoRank\")\n",
    "\n",
    "# Loop over each rank and its indices\n",
    "for rank, indices in enumerate(total_pareto_ranks_indices):\n",
    "    # Update 'ParetoRank' for the rows at the current indices to the current rank\n",
    "    df_copy.iloc[indices, pareto_rank_col_index] = rank + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new figure and axes\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Normalize color gradient based on num_ranks, be careful how it is defined in the code\n",
    "num_ranks = df_copy[\"ParetoRank\"].nunique()\n",
    "norm = plt.cm.colors.Normalize(vmin=0, vmax=num_ranks - 1)\n",
    "\n",
    "# Set the title\n",
    "ax.set_title(f\"All Pareto Ranks ({title_suffix})\")\n",
    "\n",
    "# Use the axes for the scatter plots\n",
    "ax.scatter(\n",
    "    df_copy[\"r_i_docking_score\"],\n",
    "    df_copy[\"Total_E\"],\n",
    "    color=\"lightgrey\",\n",
    "    label=\"Baseline Data\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "for i in range(1, num_ranks + 1):\n",
    "    rank_data = df_copy[df_copy[\"ParetoRank\"] == i]\n",
    "    ax.scatter(\n",
    "        rank_data[\"r_i_docking_score\"],\n",
    "        rank_data[\"Total_E\"],\n",
    "        color=colormap(norm(i - 1)),\n",
    "        label=f\"Rank {i}\",\n",
    "    )\n",
    "\n",
    "# Use the axes for the colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap=colormap, norm=norm)\n",
    "sm.set_array([])\n",
    "# Reduce the number of ticks on the colorbar\n",
    "cbar = fig.colorbar(sm, ax=ax, ticks=np.linspace(0, num_ranks - 1, num_ranks // 5))\n",
    "cbar.ax.set_yticklabels([f\"Rank {i*5+1}\" for i in range(num_ranks // 5)])\n",
    "cbar.set_label(\"Pareto Front Rank\")\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Indices of Rank 1, Pareto Front:\")\n",
    "print(total_pareto_ranks_indices[:][0])\n",
    "\n",
    "print(\"Updated Dataframe with Pareto Ranks:\")\n",
    "display(\n",
    "    df_copy[df_copy[\"ParetoRank\"] == 1][\n",
    "        [\"Molecule_Name\", \"ParetoRank\", \"r_i_docking_score\", \"Total_E\", \"Activity\"]\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Original Dataframe:\")\n",
    "display(\n",
    "    data.iloc[total_pareto_ranks_indices[:][0]][\n",
    "        [\"Molecule_Name\", \"r_i_docking_score\", \"Total_E\", \"Activity\"]\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Ranks appear to be updated correctly, plot matches and data matches in df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Now we need to get enrichment metrics as well as save them to file.\n",
    "\n",
    "  For Enrichment: We sort by pareto rank, but this metric doesn't really make sense in this context.\n",
    "\n",
    "  For ROC, we use Rank as our score. Let's do this first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pareto_rank_semi_log_roc(df, a=1e-3, ax=None):\n",
    "    # Baseline scores\n",
    "    y_scores_baseline = -df[\"r_i_docking_score\"]\n",
    "    fpr_baseline, tpr_baseline, _ = roc_curve(df[\"Activity\"], y_scores_baseline)\n",
    "    valid_indices_baseline = np.where(fpr_baseline >= a)\n",
    "    log_fpr_valid_baseline = np.log10(fpr_baseline[valid_indices_baseline])\n",
    "    log_auc_baseline = (\n",
    "        auc(log_fpr_valid_baseline, tpr_baseline[valid_indices_baseline]) / -np.log10(a)\n",
    "    ) * 10\n",
    "    ax.plot(\n",
    "        log_fpr_valid_baseline,\n",
    "        tpr_baseline[valid_indices_baseline],\n",
    "        label=f\"r_i_docking_score LogAUC (x10): {log_auc_baseline:.2f}\",\n",
    "        color=\"red\",\n",
    "    )\n",
    "\n",
    "    # New scores\n",
    "    y_scores_new = df[\"ParetoRank\"]\n",
    "    fpr_new, tpr_new, _ = roc_curve(df[\"Activity\"], y_scores_new)\n",
    "    valid_indices_new = np.where(fpr_new >= a)\n",
    "    log_fpr_valid_new = np.log10(fpr_new[valid_indices_new])\n",
    "    log_auc_new = (\n",
    "        auc(log_fpr_valid_new, tpr_new[valid_indices_new]) / -np.log10(a)\n",
    "    ) * 10\n",
    "    ax.plot(\n",
    "        log_fpr_valid_new,\n",
    "        tpr_new[valid_indices_new],\n",
    "        label=f\"ParetoRank LogAUC (x10): {log_auc_new:.2f}\",\n",
    "        color=\"blue\",\n",
    "    )\n",
    "\n",
    "    deltaLogAUC = log_auc_new - log_auc_baseline\n",
    "    print(f\"Delta LogAUC (x10): {deltaLogAUC:.2f}\")\n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_title(\n",
    "        f\"SemiLogX ROC Curve Comparison for r_i_docking_score and ParetoRank ({title_suffix})\"\n",
    "    )\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "plot_pareto_rank_semi_log_roc(df_copy, a=1e-3, ax=ax)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pareto_rank_roc(df, ax=None):\n",
    "    # Baseline scores\n",
    "    y_scores_baseline = -df[\"r_i_docking_score\"]\n",
    "    fpr_baseline, tpr_baseline, _ = roc_curve(df[\"Activity\"], y_scores_baseline)\n",
    "    roc_auc_baseline = auc(fpr_baseline, tpr_baseline)\n",
    "    ax.plot(\n",
    "        fpr_baseline,\n",
    "        tpr_baseline,\n",
    "        label=f\"r_i_docking_score ROC AUC: {roc_auc_baseline:.2f}\",\n",
    "        color=\"red\",\n",
    "    )\n",
    "\n",
    "    # New scores\n",
    "    y_scores_new = df[\"ParetoRank\"]\n",
    "    fpr_new, tpr_new, _ = roc_curve(df[\"Activity\"], y_scores_new)\n",
    "    roc_auc_new = auc(fpr_new, tpr_new)\n",
    "    ax.plot(\n",
    "        fpr_new,\n",
    "        tpr_new,\n",
    "        label=f\"ParetoRank ROC AUC: {roc_auc_new:.2f}\",\n",
    "        color=\"blue\",\n",
    "    )\n",
    "\n",
    "    deltaROC = roc_auc_new - roc_auc_baseline\n",
    "    print(f\"Delta ROC: {deltaROC:.2f}\")\n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_title(f\"ROC Curve Comparison for r_i_docking_score and ParetoRank\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "plot_pareto_rank_roc(df_copy, ax=ax)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pareto_rank_metrics(df, a=1e-3, title_suffix=title_suffix):\n",
    "    # initalize a dataframe to store pareto ranking metrics\n",
    "    pareto_ranking_metrics = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"Protein\",\n",
    "            \"Strain Energy Cutoff\",\n",
    "            \"ROC_AUC\",\n",
    "            \"Linear Log10 AUC (x10)\",\n",
    "            \"deltaAUC\",\n",
    "            \"Delta Linear Log10 AUC (x10)\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    y_scores_baseline_log = -df[\"r_i_docking_score\"]\n",
    "\n",
    "    fpr_baseline_log, tpr_baseline_log, _ = roc_curve(\n",
    "        df[\"Activity\"], y_scores_baseline_log\n",
    "    )\n",
    "\n",
    "    valid_indices_baseline_log = np.where(fpr_baseline_log >= a)\n",
    "\n",
    "    log_fpr_valid_baseline_log = np.log10(fpr_baseline_log[valid_indices_baseline_log])\n",
    "\n",
    "    log_auc_baseline = (\n",
    "        auc(log_fpr_valid_baseline_log, tpr_baseline_log[valid_indices_baseline_log])\n",
    "        / -np.log10(a)\n",
    "    ) * 10\n",
    "\n",
    "    # New scores\n",
    "    y_scores_new_log = df[\"ParetoRank\"]\n",
    "\n",
    "    fpr_new_log, tpr_new_log, _ = roc_curve(df[\"Activity\"], y_scores_new_log)\n",
    "\n",
    "    valid_indices_new_log = np.where(fpr_new_log >= a)\n",
    "\n",
    "    log_fpr_valid_new = np.log10(fpr_new_log[valid_indices_new_log])\n",
    "\n",
    "    log_auc_new = (\n",
    "        auc(log_fpr_valid_new, tpr_new_log[valid_indices_new_log]) / -np.log10(a)\n",
    "    ) * 10\n",
    "\n",
    "    deltaLogAUC = log_auc_new - log_auc_baseline\n",
    "\n",
    "    # Baseline scores\n",
    "    y_scores_baseline = -df[\"r_i_docking_score\"]\n",
    "    fpr_baseline, tpr_baseline, _ = roc_curve(df[\"Activity\"], y_scores_baseline)\n",
    "    roc_auc_baseline = auc(fpr_baseline, tpr_baseline)\n",
    "\n",
    "    # New scores\n",
    "    y_scores_new = df[\"ParetoRank\"]\n",
    "    fpr_new, tpr_new, _ = roc_curve(df[\"Activity\"], y_scores_new)\n",
    "    roc_auc_new = auc(fpr_new, tpr_new)\n",
    "\n",
    "    deltaROC = roc_auc_new - roc_auc_baseline\n",
    "\n",
    "    # Assign values to the dataframe\n",
    "    new_row = {\n",
    "        \"Protein\": title_suffix,\n",
    "        \"Strain Energy Cutoff\": \"ParetoRank\",\n",
    "        \"ROC_AUC\": roc_auc_new,\n",
    "        \"Linear Log10 AUC (x10)\": log_auc_new,\n",
    "        \"deltaAUC\": deltaROC,\n",
    "        \"Delta Linear Log10 AUC (x10)\": deltaLogAUC,\n",
    "    }\n",
    "\n",
    "    pareto_ranking_metrics = pd.concat(\n",
    "        [pareto_ranking_metrics, pd.DataFrame([new_row])], ignore_index=True\n",
    "    )\n",
    "\n",
    "    pareto_ranking_metrics.to_csv(\n",
    "        f\"./papermill/ParetoRankCSV/ParetoRanking_metrics_{title_suffix}.csv\",\n",
    "        index=False,\n",
    "    )\n",
    "\n",
    "    return pareto_ranking_metrics\n",
    "\n",
    "\n",
    "write_pareto_rank_metrics(df_copy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_ranking_metrics = write_pareto_rank_metrics(df_copy)\n",
    "display(pareto_ranking_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
