{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "(9137, 37)\n",
      "(9107, 37)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from rdkit import Chem\n",
    "from matplotlib.cm import viridis\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "matplotlib.rcdefaults()\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# Graphing Settings\n",
    "matplotlib.rcdefaults()\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# Papermill Parameters\n",
    "# title suffix\n",
    "title_suffix = \"S1PR1\"\n",
    "\n",
    "# Files we are processing\n",
    "file_path_sdf_active = \"./S1PR1/docking/S1PR1_active_docking_lib_sorted.sdf\"\n",
    "file_path_sdf_decoy = \"./S1PR1/docking/S1PR1_decoy_docking_lib_sorted.sdf\"\n",
    "file_path_strain_active = \"./S1PR1/strain/S1PR1_active_docking_lib_sorted.csv\"\n",
    "file_path_strain_decoy = \"./S1PR1/strain/S1PR1_decoy_docking_lib_sorted.csv\"\n",
    "\n",
    "\n",
    "# Data Processing\n",
    "def sdf_to_df(args):\n",
    "    \"\"\"\n",
    "    Load molecules and their properties from an SDF file into a dataframe.\n",
    "\n",
    "    Example usage: df=sdf_to_df((\"./ADRB1/docking/ADRB1_active_docking_lib.sdf\", \"active\"))\n",
    "\n",
    "    Note that this function was originally intended to be used with the `multiprocessing` module, so the arguments are passed as a tuple.\n",
    "\n",
    "    Will likely be updated later.\n",
    "\n",
    "    \"\"\"\n",
    "    file, active_status = args  # Unpack the tuple of arguments\n",
    "\n",
    "    # Create a molecule supplier\n",
    "    mol_supplier = Chem.MultithreadedSDMolSupplier(file, numWriterThreads=8)\n",
    "\n",
    "    # Load the molecules and their properties into a list\n",
    "    molecules = []\n",
    "    first_mol = True\n",
    "    for mol in mol_supplier:\n",
    "        if mol is not None:\n",
    "            if first_mol:\n",
    "                # Get properties as dictionary only for the first molecule\n",
    "                props = mol.GetPropsAsDict()\n",
    "                keys = props.keys()\n",
    "                first_mol = False\n",
    "            else:\n",
    "                # For the rest of the molecules, get properties directly\n",
    "                props = {key: mol.GetProp(key) for key in keys if mol.HasProp(key)}\n",
    "\n",
    "            props[\"Title\"] = mol.GetProp(\"_Name\")\n",
    "            props[\"Mol\"] = mol\n",
    "            props[\"Activity\"] = 1 if active_status == \"active\" else 0\n",
    "            molecules.append(props)\n",
    "\n",
    "    # Convert the list into a dataframe\n",
    "    df = pd.DataFrame(molecules)\n",
    "\n",
    "    # Reorder the dataframe columns\n",
    "    cols = [\"Title\", \"Mol\", \"Activity\"] + [\n",
    "        col for col in df.columns if col not in [\"Title\", \"Mol\", \"Activity\"]\n",
    "    ]\n",
    "    df = df[cols]\n",
    "    df = df.rename(columns={\"Title\": \"Molecule_Name\"})\n",
    "\n",
    "    # Convert 'r_i_docking_score' to numeric, coercing errors to NaN\n",
    "    df[\"r_i_docking_score\"] = pd.to_numeric(df[\"r_i_docking_score\"], errors=\"coerce\")\n",
    "\n",
    "    # Print 'Molecule_Name' and 'r_i_docking_score' for entries that could not be converted\n",
    "    non_convertible_entries = df[df[\"r_i_docking_score\"].isna()]\n",
    "    for _, row in non_convertible_entries.iterrows():\n",
    "        print(\n",
    "            f\"Molecule_Name: {row['Molecule_Name']}, r_i_docking_score: {row['r_i_docking_score']}\"\n",
    "        )\n",
    "\n",
    "    # Drop rows with non-convertible 'r_i_docking_score'\n",
    "    df = df.dropna(subset=[\"r_i_docking_score\"])\n",
    "\n",
    "    # Convert 'r_i_docking_score' to int64\n",
    "    df[\"r_i_docking_score\"] = df[\"r_i_docking_score\"].astype(\"float64\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "active_sdf = sdf_to_df((file_path_sdf_active, \"active\"))\n",
    "decoy_sdf = sdf_to_df((file_path_sdf_decoy, \"decoy\"))\n",
    "# TODO replace with a check for duplicates and sys exit with message\n",
    "duplicates_actives = active_sdf[\"Molecule_Name\"].duplicated()\n",
    "print(any(duplicates_actives))\n",
    "duplicates_decoys = decoy_sdf[\"Molecule_Name\"].duplicated()\n",
    "print(any(duplicates_decoys))\n",
    "\n",
    "\n",
    "def concatenate_csv_files(file_list):\n",
    "    \"\"\"\n",
    "    Concatenates multiple strain CSV files into a single dataframe.\n",
    "    Only the first five columns are kept for now.\n",
    "\n",
    "    Args:\n",
    "        file_list (list): A list of file paths to the CSV files.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The concatenated dataframe.\n",
    "\n",
    "    \"\"\"\n",
    "    # Specify the column names\n",
    "    column_names = [\n",
    "        \"Molecule_Name\",\n",
    "        \"Total_E\",\n",
    "        \"Lower_Bound\",\n",
    "        \"Upper_Bound\",\n",
    "        \"Num_Torsion_Patterns\",\n",
    "    ]\n",
    "\n",
    "    # List to hold dataframes\n",
    "    df_list = []\n",
    "\n",
    "    # Loop over each file in the list\n",
    "    for file in file_list:\n",
    "        # Import the CSV file as a df, using only the first five columns of the CSV file\n",
    "        df = pd.read_csv(file, usecols=range(5), names=column_names, header=0)\n",
    "        df_list.append(df)\n",
    "\n",
    "    # Concatenate all dataframes in the list\n",
    "    final_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "active_strain = concatenate_csv_files([file_path_strain_active])\n",
    "decoy_strain = concatenate_csv_files([file_path_strain_decoy])\n",
    "# TODO replace with a check for duplicates and sys exit with message\n",
    "duplicates_actives = active_strain[\"Molecule_Name\"].duplicated()\n",
    "print(any(duplicates_actives))\n",
    "duplicates_decoys = decoy_strain[\"Molecule_Name\"].duplicated()\n",
    "print(any(duplicates_decoys))\n",
    "# only keep as commented for debug\n",
    "# pre_merge = [active_sdf, decoy_sdf, active_strain, decoy_strain]\n",
    "\n",
    "# for df in pre_merge:\n",
    "#     print(df.shape)\n",
    "active_data = pd.merge(active_sdf, active_strain, on=\"Molecule_Name\")\n",
    "decoy_data = pd.merge(decoy_sdf, decoy_strain, on=\"Molecule_Name\")\n",
    "# #only keep as commented for debug\n",
    "# post_merge = [active_data, decoy_data]\n",
    "\n",
    "# for df in post_merge:\n",
    "#     print(df.shape)\n",
    "all_data = pd.concat([active_data, decoy_data])\n",
    "\n",
    "print(all_data.shape)\n",
    "all_data = all_data[all_data[\"Total_E\"] >= 0]\n",
    "\n",
    "print(all_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_pareto_ranks(scores):\n",
    "    ranks = []\n",
    "    remaining_scores = scores.copy()\n",
    "    remaining_indices = np.arange(scores.shape[0])\n",
    "    while remaining_scores.shape[0] > 0:\n",
    "        pareto_indices = identify_pareto(remaining_scores)\n",
    "        ranks.append(remaining_indices[pareto_indices])\n",
    "        remaining_scores = np.delete(remaining_scores, pareto_indices, axis=0)\n",
    "        remaining_indices = np.delete(remaining_indices, pareto_indices)\n",
    "    return ranks\n",
    "\n",
    "\n",
    "def identify_pareto(scores):\n",
    "    # Identical to the previously provided identify_pareto function\n",
    "    is_dominated = np.zeros(scores.shape[0], dtype=bool)\n",
    "    for i in range(scores.shape[0]):\n",
    "        for j in range(scores.shape[0]):\n",
    "            if all(scores[j] <= scores[i]) and any(scores[j] < scores[i]):\n",
    "                is_dominated[i] = True\n",
    "                break\n",
    "    return np.where(~is_dominated)[0]\n",
    "\n",
    "\n",
    "def find_pareto_ranks_optimized(scores):\n",
    "    ranks = []\n",
    "    remaining_scores = scores.copy()\n",
    "    remaining_indices = np.arange(scores.shape[0])\n",
    "    is_remaining = np.ones(\n",
    "        scores.shape[0], dtype=bool\n",
    "    )  # Tracks which items are still in consideration\n",
    "\n",
    "    while np.any(is_remaining):\n",
    "        pareto_indices = identify_pareto(remaining_scores[is_remaining])\n",
    "        ranks.append(remaining_indices[is_remaining][pareto_indices])\n",
    "\n",
    "        # Update the is_remaining array to exclude the identified Pareto front\n",
    "        is_remaining[is_remaining] = ~np.isin(np.where(is_remaining)[0], pareto_indices)\n",
    "\n",
    "        # No need to delete elements from remaining_scores or remaining_indices,\n",
    "        # is_remaining array efficiently manages which elements are considered in the next iteration.\n",
    "\n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = all_data.copy()\n",
    "\n",
    "data = scores[[\"r_i_docking_score\", \"Total_E\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original method execution time: 84.1769540309906 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal method execution time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 7\u001b[0m optimized_pareto_ranks_indices \u001b[38;5;241m=\u001b[39m find_pareto_ranks_optimized(data)\n\u001b[1;32m      8\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimized method execution time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 33\u001b[0m, in \u001b[0;36mfind_pareto_ranks_optimized\u001b[0;34m(scores)\u001b[0m\n\u001b[1;32m     28\u001b[0m is_remaining \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(\n\u001b[1;32m     29\u001b[0m     scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m\n\u001b[1;32m     30\u001b[0m )  \u001b[38;5;66;03m# Tracks which items are still in consideration\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(is_remaining):\n\u001b[0;32m---> 33\u001b[0m     pareto_indices \u001b[38;5;241m=\u001b[39m identify_pareto(remaining_scores[is_remaining])\n\u001b[1;32m     34\u001b[0m     ranks\u001b[38;5;241m.\u001b[39mappend(remaining_indices[is_remaining][pareto_indices])\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Update the is_remaining array to exclude the identified Pareto front\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m, in \u001b[0;36midentify_pareto\u001b[0;34m(scores)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m---> 18\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(scores[j] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m scores[i]) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(scores[j] \u001b[38;5;241m<\u001b[39m scores[i]):\n\u001b[1;32m     19\u001b[0m             is_dominated[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     20\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "all_pareto_ranks_indices = find_all_pareto_ranks(data)\n",
    "end_time = time.time()\n",
    "print(f\"Original method execution time: {end_time - start_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "optimized_pareto_ranks_indices = find_pareto_ranks_optimized(data)\n",
    "end_time = time.time()\n",
    "print(f\"Optimized method execution time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analytics_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
